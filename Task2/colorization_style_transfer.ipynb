{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f294ee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading perceptual weights from ../models/model_perceptual_weights.pth\n",
      "Selected C:/Users/NADER KAREEM/Pictures/Screenshots/newss.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --------------------\n",
    "# U-Net colorization\n",
    "# --------------------\n",
    "class UNetColorization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetColorization, self).__init__()\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec3 = nn.Conv2d(256, 3, 3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        d1 = self.dec1(e3)\n",
    "        d2 = self.dec2(torch.cat([d1, e2], dim=1))\n",
    "        d3 = self.dec3(torch.cat([d2, e1], dim=1))\n",
    "        return self.tanh(d3)\n",
    "\n",
    "# --------------------\n",
    "# VGG Style Features\n",
    "# --------------------\n",
    "class VGGStyleLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGStyleLoss, self).__init__()\n",
    "        vgg = torchvision.models.vgg19(weights=torchvision..VGG19_Weights.DEFAULT).features.to(device).eval()\n",
    "        self.layers = {\n",
    "            '0': 'conv1_1',\n",
    "            '5': 'conv2_2',\n",
    "            '10': 'conv3_2',\n",
    "            '19': 'conv4_2',\n",
    "            '28': 'conv5_2'\n",
    "        }\n",
    "        self.model = nn.ModuleDict({name: nn.Sequential() for name in self.layers.values()})\n",
    "        current_name = None\n",
    "        for i, layer in enumerate(vgg.children()):\n",
    "            for key, name in self.layers.items():\n",
    "                if str(i) == key:\n",
    "                    current_name = name\n",
    "            if current_name is not None:\n",
    "                if isinstance(layer, nn.ReLU):\n",
    "                    self.model[current_name].add_module(str(len(self.model[current_name])), nn.ReLU(inplace=False))\n",
    "                else:\n",
    "                    self.model[current_name].add_module(str(len(self.model[current_name])), layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = {}\n",
    "        out = x\n",
    "        for name, module in self.model.items():\n",
    "            for layer in module:\n",
    "                out = layer(out)\n",
    "            features[name] = out.clone()\n",
    "        return features\n",
    "\n",
    "# --------------------\n",
    "# Visualization Effects\n",
    "# --------------------\n",
    "def torch_rgb_to_hsv(rgb):\n",
    "    r, g, b = rgb[:, 0, :, :], rgb[:, 1, :, :], rgb[:, 2, :, :]\n",
    "    max_val, _ = torch.max(rgb, dim=1)\n",
    "    min_val, _ = torch.min(rgb, dim=1)\n",
    "    diff = max_val - min_val\n",
    "    h = torch.zeros_like(r)\n",
    "    mask = (max_val == r) & (g >= b)\n",
    "    h[mask] = (g[mask] - b[mask]) / diff[mask]\n",
    "    mask = (max_val == r) & (g < b)\n",
    "    h[mask] = (g[mask] - b[mask]) / diff[mask] + 6.0\n",
    "    mask = max_val == g\n",
    "    h[mask] = (b[mask] - r[mask]) / diff[mask] + 2.0\n",
    "    mask = max_val == b\n",
    "    h[mask] = (r[mask] - g[mask]) / diff[mask] + 4.0\n",
    "    h = h / 6.0\n",
    "    h[diff == 0.0] = 0.0\n",
    "    s = torch.zeros_like(r)\n",
    "    s[diff != 0.0] = diff[diff != 0.0] / max_val[diff != 0.0]\n",
    "    v = max_val\n",
    "    return torch.stack([h, s, v], dim=1)\n",
    "\n",
    "def torch_hsv_to_rgb(hsv):\n",
    "    h, s, v = hsv[:, 0, :, :], hsv[:, 1, :, :], hsv[:, 2, :, :]\n",
    "    i = (h * 6.0).floor()\n",
    "    f = h * 6.0 - i\n",
    "    p = v * (1.0 - s)\n",
    "    q = v * (1.0 - s * f)\n",
    "    t = v * (1.0 - s * (1.0 - f))\n",
    "    i_mod = i % 6\n",
    "    r = torch.zeros_like(h); g = torch.zeros_like(h); b = torch.zeros_like(h)\n",
    "    r[i_mod == 0.0] = v[i_mod == 0.0]; g[i_mod == 0.0] = t[i_mod == 0.0]; b[i_mod == 0.0] = p[i_mod == 0.0]\n",
    "    r[i_mod == 1.0] = q[i_mod == 1.0]; g[i_mod == 1.0] = v[i_mod == 1.0]; b[i_mod == 1.0] = p[i_mod == 1.0]\n",
    "    r[i_mod == 2.0] = p[i_mod == 2.0]; g[i_mod == 2.0] = v[i_mod == 2.0]; b[i_mod == 2.0] = t[i_mod == 2.0]\n",
    "    r[i_mod == 3.0] = p[i_mod == 3.0]; g[i_mod == 3.0] = q[i_mod == 3.0]; b[i_mod == 3.0] = v[i_mod == 3.0]\n",
    "    r[i_mod == 4.0] = t[i_mod == 4.0]; g[i_mod == 4.0] = p[i_mod == 4.0]; b[i_mod == 4.0] = v[i_mod == 4.0]\n",
    "    r[i_mod == 5.0] = v[i_mod == 5.0]; g[i_mod == 5.0] = p[i_mod == 5.0]; b[i_mod == 5.0] = q[i_mod == 5.0]\n",
    "    return torch.stack([r, g, b], dim=1)\n",
    "\n",
    "def exaggerate_colors(images, saturation_factor=1.5, value_factor=1.2):\n",
    "    images = (images + 1) / 2.0\n",
    "    images_hsv = torch_rgb_to_hsv(images)\n",
    "    images_hsv[:, 1, :, :] = torch.clamp(images_hsv[:, 1, :, :] * saturation_factor, 0, 1)\n",
    "    images_hsv[:, 2, :, :] = torch.clamp(images_hsv[:, 2, :, :] * value_factor, 0, 1)\n",
    "    color_exaggerated_images = torch_hsv_to_rgb(images_hsv)\n",
    "    color_exaggerated_images = color_exaggerated_images * 2.0 - 1.0\n",
    "    return color_exaggerated_images\n",
    "\n",
    "# --------------------\n",
    "# Helper functions\n",
    "# --------------------\n",
    "def gram_matrix(tensor):\n",
    "    b, c, h, w = tensor.size()\n",
    "    features = tensor.view(b * c, h * w)\n",
    "    gram = torch.mm(features, features.t())\n",
    "    return gram.div(b * c * h * w)\n",
    "\n",
    "def load_style_image(style_path, size=(256, 256)):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    image = Image.open(style_path).convert('RGB')\n",
    "    return transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "def preprocess_image(image_path, size=(256, 256)):\n",
    "    transform_gray = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    transform_low = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),  # Increased from 32x32 to 128x128\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('L')\n",
    "    low = transform_low(image).unsqueeze(0).to(device)\n",
    "    high = transform_gray(image).unsqueeze(0).to(device)\n",
    "    return low, high\n",
    "\n",
    "def postprocess_image(tensor):\n",
    "    tensor = tensor.cpu().clamp(-1, 1)\n",
    "    tensor = (tensor + 1) / 2\n",
    "    np_img = tensor.squeeze(0).permute(1, 2, 0).numpy()\n",
    "    return Image.fromarray((np_img * 255).astype(np.uint8))\n",
    "\n",
    "def apply_style_transfer(colorized, style_image, vgg, content_weight=1e2, style_weight=1e7, steps=200):\n",
    "    opt_img = colorized.detach().clone().requires_grad_(True)\n",
    "    optimizer = optim.Adam([opt_img], lr=0.01)\n",
    "    content_features = vgg(colorized)\n",
    "    style_features = vgg(style_image)\n",
    "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "\n",
    "    for _ in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "        out_features = vgg(opt_img)\n",
    "        content_loss = torch.mean((out_features['conv4_2'] - content_features['conv4_2']) ** 2)\n",
    "        style_loss = 0\n",
    "        for layer in style_grams:\n",
    "            gm_out = gram_matrix(out_features[layer])\n",
    "            gm_style = style_grams[layer]\n",
    "            style_loss += torch.mean((gm_out - gm_style) ** 2)\n",
    "        style_loss /= len(style_grams)\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        total_loss.backward(retain_graph=True)  # Retain graph to allow multiple backward passes\n",
    "        optimizer.step()\n",
    "    return opt_img.detach()\n",
    "\n",
    "# --------------------\n",
    "# GUI Class\n",
    "# --------------------\n",
    "style_images = {\n",
    "    'Van Gogh': 'vangogh.jpg',\n",
    "    'Monet': 'monet.jpg',\n",
    "    'Ukiyo-e': 'ukiyoe.jpg'\n",
    "}\n",
    "\n",
    "class ColorizationGUI:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Image Colorization + Style Transfer with Enhanced Effects\")\n",
    "        self.model = UNetColorization().to(device)\n",
    "        self.vgg = VGGStyleLoss().to(device)\n",
    "        self.load_model()\n",
    "        try:\n",
    "            self.styles = {name: load_style_image(path) for name, path in style_images.items()}\n",
    "        except FileNotFoundError as e:\n",
    "            messagebox.showerror(\"Error\", f\"Missing style image: {e}\")\n",
    "            self.root.quit()\n",
    "\n",
    "        tk.Label(root, text=\"Select grayscale image and style:\").pack()\n",
    "        self.style_var = tk.StringVar(value=list(style_images.keys())[0])\n",
    "        tk.OptionMenu(root, self.style_var, *style_images.keys()).pack()\n",
    "        tk.Button(root, text=\"Upload\", command=self.upload_image).pack()\n",
    "        tk.Button(root, text=\"Colorize & Style\", command=self.process_image).pack()\n",
    "        tk.Button(root, text=\"Save Output\", command=self.save_image).pack()\n",
    "\n",
    "        self.image_path = None\n",
    "        self.output_image = None\n",
    "\n",
    "    def load_model(self):\n",
    "        weights_path = '../models/model_perceptual_weights.pth'\n",
    "        if os.path.exists(weights_path):\n",
    "            print(f\"Loading perceptual weights from {weights_path}\")\n",
    "            state_dict = torch.load(weights_path, map_location=device)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", f\"Cannot find {weights_path}. Please train the perceptual model using Code 2 first.\")\n",
    "            self.root.quit()\n",
    "        self.model.eval()\n",
    "\n",
    "    def upload_image(self):\n",
    "        self.image_path = filedialog.askopenfilename(filetypes=[(\"Image files\",\"*.png *.jpg *.jpeg\")])\n",
    "        if self.image_path:\n",
    "            print(f\"Selected {self.image_path}\")\n",
    "\n",
    "    def process_image(self):\n",
    "        if not self.image_path:\n",
    "            messagebox.showerror(\"Error\", \"No image selected!\")\n",
    "            return\n",
    "        low_res, _ = preprocess_image(self.image_path)\n",
    "        with torch.no_grad():\n",
    "            colorized_low = self.model(low_res)\n",
    "            colorized = torch.nn.functional.interpolate(colorized_low, size=(512, 512), mode='bilinear')\n",
    "            # Apply color exaggeration\n",
    "            colorized = exaggerate_colors(colorized, saturation_factor=1.5, value_factor=1.2)\n",
    "        style_name = self.style_var.get()\n",
    "        styled = apply_style_transfer(colorized, self.styles[style_name], self.vgg)\n",
    "        self.output_image = postprocess_image(styled)\n",
    "        self.output_image.show()\n",
    "\n",
    "    def save_image(self):\n",
    "        if self.output_image:\n",
    "            path = filedialog.asksaveasfilename(defaultextension=\".png\")\n",
    "            if path:\n",
    "                self.output_image.save(path)\n",
    "                messagebox.showinfo(\"Saved\", f\"Output saved to {path}\")\n",
    "\n",
    "# --------------------\n",
    "# Run\n",
    "# --------------------\n",
    "def main():\n",
    "    root = tk.Tk()\n",
    "    app = ColorizationGUI(root)\n",
    "    root.mainloop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
