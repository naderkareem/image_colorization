{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb526bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 5.57M/170M [00:04<02:13, 1.24MB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Load CIFAR-10 dataset\u001b[39;00m\n\u001b[32m     89\u001b[39m transform = transforms.Compose([\n\u001b[32m     90\u001b[39m     transforms.ToTensor(),\n\u001b[32m     91\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m train_dataset = \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m test_dataset = torchvision.datasets.CIFAR10(root=\u001b[33m'\u001b[39m\u001b[33m./data\u001b[39m\u001b[33m'\u001b[39m, train=\u001b[38;5;28;01mFalse\u001b[39;00m, download=\u001b[38;5;28;01mTrue\u001b[39;00m, transform=transform)\n\u001b[32m     96\u001b[39m train_loader = DataLoader(train_dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\datasets\\cifar.py:66\u001b[39m, in \u001b[36mCIFAR10.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.train = train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\datasets\\cifar.py:139\u001b[39m, in \u001b[36mCIFAR10.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\datasets\\utils.py:391\u001b[39m, in \u001b[36mdownload_and_extract_archive\u001b[39m\u001b[34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[32m    389\u001b[39m     filename = os.path.basename(url)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m archive = os.path.join(download_root, filename)\n\u001b[32m    394\u001b[39m extract_archive(archive, extract_root, remove_finished)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\datasets\\utils.py:130\u001b[39m, in \u001b[36mdownload_url\u001b[39m\u001b[34m(url, root, filename, md5, max_redirect_hops)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib.error.URLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[32m5\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mhttps\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\datasets\\utils.py:30\u001b[39m, in \u001b[36m_urlretrieve\u001b[39m\u001b[34m(url, filename, chunk_size)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m urllib.request.urlopen(urllib.request.Request(url, headers={\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total=response.length, unit=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m, unit_scale=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk := \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     31\u001b[39m             fh.write(chunk)\n\u001b[32m     32\u001b[39m             pbar.update(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\anaconda\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import lpips\n",
    "import os\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch.serialization\n",
    "\n",
    "# Configuration\n",
    "NUM_BINS = 2  # Use 2 bins for accuracy calculation\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define U-Net for colorization\n",
    "class UNetColorization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNetColorization, self).__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec3 = nn.Conv2d(256, 3, 3, padding=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(e1)\n",
    "        e3 = self.enc3(e2)\n",
    "        d1 = self.dec1(e3)\n",
    "        d2 = self.dec2(torch.cat([d1, e2], dim=1))\n",
    "        d3 = self.dec3(torch.cat([d2, e1], dim=1))\n",
    "        return self.tanh(d3)\n",
    "\n",
    "# Allowlist necessary modules for safe model loading\n",
    "torch.serialization.add_safe_globals([\n",
    "    UNetColorization,\n",
    "    torch.nn.Sequential,\n",
    "    torch.nn.Conv2d,\n",
    "    torch.nn.BatchNorm2d,\n",
    "    torch.nn.Dropout,\n",
    "    torch.nn.ReLU,\n",
    "    torch.nn.ConvTranspose2d,\n",
    "    torch.nn.Tanh\n",
    "])\n",
    "\n",
    "# Make sure models folder exists in current directory\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Function to convert RGB to grayscale\n",
    "def rgb_to_gray(img):\n",
    "    return img.mean(dim=1, keepdim=True)\n",
    "\n",
    "# Perceptual Loss using LPIPS\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, loss_type, epochs=30):\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    mse_loss = nn.MSELoss()\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, _) in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')):\n",
    "            images = images.to(device)\n",
    "            grayscale_images = rgb_to_gray(images).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(grayscale_images)\n",
    "            if loss_type == 'mse':\n",
    "                loss = mse_loss(outputs, images)\n",
    "            elif loss_type == 'perceptual':\n",
    "                loss = loss_fn_vgg(outputs, images).mean()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        print(f'Epoch {epoch+1}, {loss_type.upper()} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "# Evaluation function with per-channel accuracy\n",
    "def evaluate_model(model, test_loader, loss_type):\n",
    "    model.eval()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    total_mse = 0.0\n",
    "    total_perceptual = 0.0\n",
    "    total_psnr = 0.0\n",
    "    total_ssim = 0.0\n",
    "    all_accuracies = []\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(test_loader, desc=f'Evaluating {loss_type}'):\n",
    "            images = images.to(device)\n",
    "            grayscale = rgb_to_gray(images).to(device)\n",
    "            outputs = model(grayscale)\n",
    "\n",
    "            # Compute MSE and Perceptual Loss\n",
    "            mse = mse_loss(outputs, images).item()\n",
    "            perceptual = loss_fn_vgg(outputs, images).mean().item()\n",
    "            total_mse += mse * images.size(0)\n",
    "            total_perceptual += perceptual * images.size(0)\n",
    "\n",
    "            # Compute PSNR and SSIM\n",
    "            images_np = images.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "            outputs_np = outputs.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "            for i in range(images_np.shape[0]):\n",
    "                total_psnr += psnr(images_np[i], outputs_np[i], data_range=1.0)\n",
    "                total_ssim += ssim(images_np[i], outputs_np[i], data_range=1.0, channel_axis=-1, win_size=3)\n",
    "            num_samples += images.size(0)\n",
    "\n",
    "            # Per-channel accuracy\n",
    "            bins = np.linspace(0, 1, NUM_BINS + 1)  # Adjusted for [0, 1] range\n",
    "            for c in range(3):  # R, G, B channels\n",
    "                preds_c = outputs[:, c, :, :].cpu().numpy().flatten()\n",
    "                targets_c = images[:, c, :, :].cpu().numpy().flatten()\n",
    "                preds_c = np.clip(preds_c, 0, 1)\n",
    "                targets_c = np.clip(targets_c, 0, 1)\n",
    "                preds_binned_c = np.digitize(preds_c, bins)\n",
    "                targets_binned_c = np.digitize(targets_c, bins)\n",
    "                cm_c = confusion_matrix(targets_binned_c, preds_binned_c)\n",
    "                all_accuracies.append(np.sum(np.diag(cm_c)) / np.sum(cm_c))\n",
    "\n",
    "    avg_mse = total_mse / num_samples\n",
    "    avg_perceptual = total_perceptual / num_samples\n",
    "    avg_psnr = total_psnr / num_samples\n",
    "    avg_ssim = total_ssim / num_samples\n",
    "    avg_accuracy = np.mean(all_accuracies)\n",
    "    cm = confusion_matrix(np.digitize(np.clip(outputs_np.flatten(), 0, 1), bins), np.digitize(np.clip(images_np.flatten(), 0, 1), bins))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        np.digitize(np.clip(images_np.flatten(), 0, 1), bins), np.digitize(np.clip(outputs_np.flatten(), 0, 1), bins), average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    return avg_mse, avg_perceptual, avg_psnr, avg_ssim, cm, precision, recall, f1, avg_accuracy\n",
    "\n",
    "# Visualize samples\n",
    "def visualize_samples(model, test_loader, num_samples=5, model_name='model'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images, _ = next(iter(test_loader))\n",
    "        grayscale = rgb_to_gray(images).to(device)\n",
    "        outputs = model(grayscale)\n",
    "        for i in range(num_samples):\n",
    "            plt.figure(figsize=(12, 3))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.imshow(grayscale[i, 0].cpu().numpy(), cmap='gray')\n",
    "            plt.title('Grayscale')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.imshow((images[i].cpu().numpy().transpose(1, 2, 0)))\n",
    "            plt.title('Ground Truth')\n",
    "            plt.axis('off')\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.imshow((outputs[i].cpu().numpy().transpose(1, 2, 0)))\n",
    "            plt.title('Predicted')\n",
    "            plt.axis('off')\n",
    "            plt.savefig(f'models/sample_{i}_{model_name}.png')\n",
    "            plt.show()\n",
    "\n",
    "# Check if models exist, load or train\n",
    "def load_or_train_model(model, model_path, weights_path, train_loader, loss_type):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f'Loading existing {loss_type.upper()} model...')\n",
    "        try:\n",
    "            model = torch.load(model_path)\n",
    "            return model, []\n",
    "        except Exception as e:\n",
    "            print(f'Failed to load model: {e}. Loading weights instead...')\n",
    "            model.load_state_dict(torch.load(weights_path))\n",
    "            return model, []\n",
    "    else:\n",
    "        print(f'Training {loss_type.upper()} model...')\n",
    "        losses = train_model(model, train_loader, loss_type)\n",
    "        torch.save(model.state_dict(), weights_path)\n",
    "        torch.save(model, model_path)\n",
    "        return model, losses\n",
    "\n",
    "# Paths to save models inside ./models folder\n",
    "mse_model_path = '../models/model_mse.pth'\n",
    "mse_weights_path = '../models/model_mse_weights.pth'\n",
    "perceptual_model_path = '../models/model_perceptual.pth'\n",
    "perceptual_weights_path = '../models/model_perceptual_weights.pth'\n",
    "\n",
    "# MSE model\n",
    "model_mse = UNetColorization()\n",
    "model_mse, mse_losses = load_or_train_model(model_mse, mse_model_path, mse_weights_path, train_loader, 'mse')\n",
    "# Evaluate MSE model\n",
    "mse_metrics = evaluate_model(model_mse, test_loader, 'mse')\n",
    "print(f'MSE Model - MSE Loss: {mse_metrics[0]:.4f}, Perceptual Loss: {mse_metrics[1]:.4f}, PSNR: {mse_metrics[2]:.4f}, SSIM: {mse_metrics[3]:.4f}')\n",
    "print(f'Accuracy: {mse_metrics[8]:.4f}, Precision: {mse_metrics[5]:.4f}, Recall: {mse_metrics[6]:.4f}, F1: {mse_metrics[7]:.4f}')\n",
    "visualize_samples(model_mse, test_loader, model_name='mse')\n",
    "\n",
    "# Perceptual model\n",
    "model_perceptual = UNetColorization()\n",
    "model_perceptual, perceptual_losses = load_or_train_model(model_perceptual, perceptual_model_path, perceptual_weights_path, train_loader, 'perceptual')\n",
    "# Evaluate Perceptual model\n",
    "perceptual_metrics = evaluate_model(model_perceptual, test_loader, 'perceptual')\n",
    "print(f'Perceptual Model - MSE Loss: {perceptual_metrics[0]:.4f}, Perceptual Loss: {perceptual_metrics[1]:.4f}, PSNR: {perceptual_metrics[2]:.4f}, SSIM: {perceptual_metrics[3]:.4f}')\n",
    "print(f'Accuracy: {perceptual_metrics[8]:.4f}, Precision: {perceptual_metrics[5]:.4f}, Recall: {perceptual_metrics[6]:.4f}, F1: {perceptual_metrics[7]:.4f}')\n",
    "visualize_samples(model_perceptual, test_loader, model_name='perceptual')\n",
    "\n",
    "# Plot training losses if training occurred\n",
    "if mse_losses or perceptual_losses:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    if mse_losses:\n",
    "        plt.plot(mse_losses, label='MSE Loss')\n",
    "    if perceptual_losses:\n",
    "        plt.plot(perceptual_losses, label='Perceptual Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss Comparison')\n",
    "    plt.legend()\n",
    "    plt.savefig('models/loss_comparison.png')\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(mse_metrics[4], cmap='Blues')\n",
    "plt.title('Confusion Matrix - MSE Model')\n",
    "plt.colorbar()\n",
    "plt.savefig('models/cm_mse.png')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(perceptual_metrics[4], cmap='Blues')\n",
    "plt.title('Confusion Matrix - Perceptual Model')\n",
    "plt.colorbar()\n",
    "plt.savefig('models/cm_perceptual.png')\n",
    "plt.show()\n",
    "\n",
    "# Comparison and Explanation\n",
    "print('Comparison of Loss Functions:')\n",
    "print('MSE Loss: Optimizes pixel-wise accuracy, leading to smoother outputs but potentially less vibrant colors.')\n",
    "print('Perceptual Loss: Emphasizes high-level features, improving visual quality but possibly sacrificing pixel-wise accuracy.')\n",
    "print('U-Net with batch normalization and dropout enhances detail preservation. PSNR/SSIM are key for quality assessment.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
