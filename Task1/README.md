Image Colorization with U-Net: Loss Function Comparison
Overview
This project implements an image colorization model using a U-Net architecture to colorize grayscale images from the CIFAR-10 dataset. The model is trained with two different loss functions: Mean Squared Error (MSE) and Perceptual Loss (using LPIPS with VGG network). The code evaluates and compares the performance of these loss functions based on metrics such as MSE, Perceptual Loss, PSNR, SSIM, and per-channel accuracy.
Requirements
To run the code, ensure you have the following dependencies installed:

Python 3.8+
PyTorch
torchvision
NumPy
scikit-learn
matplotlib
tqdm
lpips
scikit-image

You can install the dependencies using:
pip install torch torchvision numpy scikit-learn matplotlib tqdm lpips scikit-image

Dataset
The code uses the CIFAR-10 dataset, which consists of 60,000 32x32 color images (50,000 for training, 10,000 for testing) across 10 classes. The dataset is automatically downloaded to the ./data directory when the script is run.
Preprocessing

Images are transformed using transforms.ToTensor() to convert them to tensors in the range [0, 1].
Grayscale images are generated by taking the mean of RGB channels (rgb_to_gray function).

Model Architecture
The model is a U-Net designed for colorization, consisting of:

Encoder: Three convolutional blocks with increasing channel sizes (1→128, 128→256, 256→512), each with batch normalization, ReLU activation, and dropout (0.4).
Decoder: Two transposed convolutional blocks (512→256, 512→128) with skip connections from the encoder, followed by a final convolutional layer to output 3 channels (RGB) with a Tanh activation.
The input is a single-channel grayscale image, and the output is a 3-channel RGB image.

Loss Functions
The model is trained with two different loss functions:

Mean Squared Error (MSE): Measures pixel-wise differences between predicted and ground truth images, optimizing for pixel accuracy.
Perceptual Loss (LPIPS): Uses a pre-trained VGG network to compute perceptual differences, focusing on high-level feature similarity for improved visual quality.

Training

Dataset: CIFAR-10 (50,000 training images, 64 batch size).
Optimizer: Adam with a learning rate of 0.001.
Epochs: 30 epochs.
Training Loop: For each epoch, the model processes grayscale images, computes the specified loss (MSE or Perceptual), and updates weights. Progress is printed every 100 steps.
Model Saving: Trained models and weights are saved as model_mse.pth, model_mse_weights.pth, model_perceptual.pth, and model_perceptual_weights.pth. If these files exist, the model is loaded instead of retrained.

Evaluation
The models are evaluated on the CIFAR-10 test set using the following metrics:

MSE Loss: Pixel-wise error.
Perceptual Loss: LPIPS score using VGG.
PSNR: Peak Signal-to-Noise Ratio for image quality.
SSIM: Structural Similarity Index for perceptual quality.
Per-Channel Accuracy: Accuracy of predicted RGB channels using 2 bins ([0, 1] range).
Confusion Matrix, Precision, Recall, F1: Computed for binned pixel values to assess classification performance.

Visualization

Sample Images: The visualize_samples function generates side-by-side plots of grayscale input, ground truth, and predicted colorized images for 5 samples, saved as PNG files (e.g., sample_0_mse.png).
Loss Curves: Training loss curves for MSE and Perceptual models are plotted and saved as loss_comparison.png if training occurs.
Confusion Matrices: Confusion matrices for both models are plotted and saved as cm_mse.png and cm_perceptual.png.

Results

MSE Loss: Produces smoother outputs with higher pixel-wise accuracy but may result in less vibrant colors.
Perceptual Loss: Enhances visual quality by preserving high-level features, potentially at the cost of pixel-wise accuracy.
Metrics Comparison: The script prints MSE, Perceptual Loss, PSNR, SSIM, Accuracy, Precision, Recall, and F1 scores for both models.
Key Observation: The U-Net architecture with batch normalization and dropout improves detail preservation. PSNR and SSIM are critical for assessing image quality.

Usage

Ensure dependencies are installed.
Run : task1.ipynb


The script will:
Download CIFAR-10 (if not already downloaded).
Train or load the MSE and Perceptual models.
Evaluate both models on the test set.
Generate visualizations and save them as PNG files.
Print a comparison of loss functions.



Output Files

Model Files: model_mse.pth, model_mse_weights.pth, model_perceptual.pth, model_perceptual_weights.pth
Visualizations: sample_[0-4]_[mse/perceptual].png, loss_comparison.png, cm_mse.png, cm_perceptual.png

Notes

The code checks for existing models to avoid retraining. Delete model files to retrain from scratch.
The dataset is switched to CIFAR-10 for consistency with the provided reference code.
The training loop adopts a simpler structure without early stopping or learning rate scheduling, mirroring the reference code.
The range for pixel values is [0, 1] due to the dataset transformation, and evaluation metrics are adjusted accordingly.
